{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Read full splits for EDA/modeling ---\n",
    "data_dir = Path(\"data/processed\")\n",
    "\n",
    "train = pd.read_parquet(data_dir / \"train.parquet\", engine=\"fastparquet\")\n",
    "valid = pd.read_parquet(data_dir / \"valid.parquet\", engine=\"fastparquet\")\n",
    "test  = pd.read_parquet(data_dir / \"test.parquet\",  engine=\"fastparquet\")\n",
    "\n",
    "# Quick sanity checks (types, shapes, date ranges)\n",
    "print(\"train:\", train.shape, train[\"issue_d\"].min(), \"→\", train[\"issue_d\"].max())\n",
    "print(\"valid:\", valid.shape, valid[\"issue_d\"].min(), \"→\", valid[\"issue_d\"].max())\n",
    "print(\"test :\", test.shape,  test[\"issue_d\"].min(),  \"→\", test[\"issue_d\"].max())\n",
    "\n",
    "# If you want a light sample for quick plots (keeps types)\n",
    "train_s = train.sample(n=min(5000, len(train)), random_state=42)\n",
    "valid_s = valid.sample(n=min(3000, len(valid)), random_state=42)\n",
    "test_s  = test.sample(n=min(3000, len(test)),  random_state=42)\n",
    "\n",
    "# Example: one combined sample for quick EDA\n",
    "eda_sample = (\n",
    "    pd.concat([\n",
    "        train_s.assign(split=\"train\"),\n",
    "        valid_s.assign(split=\"valid\"),\n",
    "        test_s.assign(split=\"test\")\n",
    "    ], ignore_index=True)\n",
    ")\n",
    "\n",
    "# Peek at dtypes to confirm they’re preserved\n",
    "print(eda_sample.dtypes.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3559a3c",
   "metadata": {},
   "source": [
    "## Erase post origination and pricing features\n",
    "\n",
    "Pricing features are eliminated to avoid circularity with pricing. So features as term and rate related features are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it simple: define a clear DROP list and remove safely if columns exist.\n",
    "DROP_COLS = [\n",
    "    # Estado & pagos\n",
    "    \"loan_status\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\",\n",
    "    \"total_rec_late_fee\", \"out_prncp\", \"out_prncp_inv\", \"recoveries\", \"collection_recovery_fee\",\n",
    "    \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\",\n",
    "    # Últimos pulls\n",
    "    \"last_credit_pull_d\", \"last_fico_range_low\", \"last_fico_range_high\",\n",
    "    # Hardship / diferimientos\n",
    "    \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\", \"deferral_term\",\n",
    "    \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\",\n",
    "    \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\",\n",
    "    \"orig_projected_additional_accrued_interest\", \"hardship_payoff_balance_amount\",\n",
    "    \"hardship_last_payment_amount\",\n",
    "    # Acuerdos de deuda\n",
    "    \"debt_settlement_flag\", \"debt_settlement_flag_date\", \"settlement_status\", \"settlement_date\",\n",
    "    \"settlement_amount\", \"settlement_percentage\", \"settlement_term\",\n",
    "    # Plan de pagos\n",
    "    \"pymnt_plan\",\n",
    "    # Progreso de fondeo (mejor excluir)\n",
    "    \"funded_amnt\", \"funded_amnt_inv\",\n",
    "    # IDs / utilitarios\n",
    "    \"id\", \"member_id\", \"url\", \"policy_code\",\n",
    "]\n",
    "\n",
    "PREFIXES_TO_DROP = (\"last_\", \"hardship_\", \"settlement_\")\n",
    "\n",
    "def filter_origin_features(df):\n",
    "    cols = [c for c in df.columns if c not in DROP_COLS and not c.startswith(PREFIXES_TO_DROP)]\n",
    "    return df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2024e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "PRICING_ARTIFACTS = [\n",
    "    \"int_rate\", \"grade\", \"sub_grade\", \"installment\",\n",
    "    \"initial_list_status\", \"funded_amnt\", \"funded_amnt_inv\",\n",
    "]\n",
    "\n",
    "POST_ORIG_BASE = [\n",
    "    \"loan_status\",\"total_pymnt\",\"total_pymnt_inv\",\"total_rec_prncp\",\"total_rec_int\",\n",
    "    \"total_rec_late_fee\",\"out_prncp\",\"out_prncp_inv\",\"recoveries\",\"collection_recovery_fee\",\n",
    "    \"last_pymnt_d\",\"last_pymnt_amnt\",\"next_pymnt_d\",\n",
    "    \"last_credit_pull_d\",\"last_fico_range_low\",\"last_fico_range_high\",\n",
    "    \"pymnt_plan\", 'deferral_term', 'orig_projected_additional_accrued_interest'\n",
    "]\n",
    "\n",
    "UTILITY_TEXT = [\"desc\", \"emp_title\", \"title\", \"url\", \"policy_code\", \"member_id\", \"id\", \"Unnamed: 0\"]\n",
    "\n",
    "FAMILY_PREFIXES: Iterable[str] = (\"hardship_\", \"settlement_\", \"debt_settlement_flag\")\n",
    "\n",
    "# Date-like names we want out even if dtype is object\n",
    "EXTRA_DATE_NAMES = {\"issue_d\", \"earliest_cr_line\", \"payment_plan_start_date\", \"year\", \"issue_year\"}\n",
    "\n",
    "def make_pre_offer_eda(df, keep_term: bool = False, keep_target: bool = True):\n",
    "    # Base drop lists\n",
    "    family_cols = [c for c in df.columns if c.startswith(FAMILY_PREFIXES)]\n",
    "    drop_cols = set(PRICING_ARTIFACTS) | set(POST_ORIG_BASE) | set(UTILITY_TEXT) | set(family_cols)\n",
    "\n",
    "    # Dynamic: drop datetime dtype columns\n",
    "    dt_cols = set(df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns)\n",
    "\n",
    "    # Dynamic: drop columns whose names look date-like\n",
    "    name_date_cols = {c for c in df.columns if c.endswith(\"_d\") or c.endswith(\"_date\")}\n",
    "    name_date_cols |= (EXTRA_DATE_NAMES & set(df.columns))\n",
    "\n",
    "    drop_cols |= dt_cols | name_date_cols\n",
    "\n",
    "    # Build keep list\n",
    "    cols_keep = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "    # Optionally drop term for strict pre-offer\n",
    "    if not keep_term and \"term\" in cols_keep:\n",
    "        cols_keep.remove(\"term\")\n",
    "\n",
    "    # Keep/Drop target as requested (default: keep)\n",
    "    if not keep_target and \"target\" in cols_keep:\n",
    "        cols_keep.remove(\"target\")\n",
    "\n",
    "    df_out = df[cols_keep].copy()\n",
    "    df_out.attrs[\"dropped_columns\"] = sorted([c for c in drop_cols if c in df.columns])\n",
    "    return df_out\n",
    "\n",
    "\n",
    "train_pre_eda = make_pre_offer_eda(train, keep_term=False, keep_target=True)\n",
    "valid_pre_eda = make_pre_offer_eda(valid, keep_term=False, keep_target=True)\n",
    "test_pre_eda  = make_pre_offer_eda(test,  keep_term=False, keep_target=True)\n",
    "\n",
    "print(\"Dropped (train):\", train_pre_eda.attrs[\"dropped_columns\"][:10], \"… total:\", len(train_pre_eda.attrs[\"dropped_columns\"]))\n",
    "print(\"Kept columns:\", len(train_pre_eda.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38417ea",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA) on train selected sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def quick_profile(df: pd.DataFrame, exclude: list | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple variable prescription (profile):\n",
    "    - dtype\n",
    "    - % NaN\n",
    "    - # unique\n",
    "    - min / p50 / max for numeric\n",
    "    - top category and its % for non-numeric\n",
    "    \"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "    cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        row = {\n",
    "            \"feature\": c,\n",
    "            \"dtype\": str(s.dtype),\n",
    "            \"nan_pct\": round(100 * s.isna().mean(), 2),\n",
    "            \"n_unique\": int(s.nunique(dropna=True)),\n",
    "        }\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            row.update({\n",
    "                \"min\": s.min(),\n",
    "                \"p50\": s.median(),\n",
    "                \"max\": s.max(),\n",
    "            })\n",
    "        else:\n",
    "            top = s.value_counts(dropna=True).head(1)\n",
    "            if not top.empty:\n",
    "                top_val = top.index[0]\n",
    "                top_pct = round(100 * top.iloc[0] / s.notna().sum(), 2) if s.notna().sum() else 0.0\n",
    "                row.update({\"top\": str(top_val), \"top_pct\": top_pct})\n",
    "        rows.append(row)\n",
    "\n",
    "    prof = pd.DataFrame(rows).sort_values([\"nan_pct\", \"feature\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    return prof\n",
    "\n",
    "# Example usage (exclude target if you want):\n",
    "profile_train = quick_profile(train_pre_eda, exclude=[\"target\"])\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "profile_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df66df9",
   "metadata": {},
   "source": [
    "For now doesn't worth to segment by application type (individual or joint) due to few information. So variables to drop are:\n",
    "\n",
    "* Those that start with sec_app_\n",
    "* Those which end with joint\n",
    "\n",
    "Thought some variables have high NaN values, some are reasonable to have it. So we are going to keep buro variables like mths_since_last_record, mths_since_last_major_derog, mths_since_recent_revol_delinq, etc. For those variables NaN are going to be imputed with 999.\n",
    "\n",
    "Some other variables representing ratios like il_util, il_util, etc, are resonable to have NaN values as they coul be informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "post_orig = {\"deferral_term\", \"orig_projected_additional_accrued_interest\"}\n",
    "\n",
    "joint_like = {\n",
    "    \"annual_inc_joint\",\"dti_joint\",\"verification_status_joint\",\"revol_bal_joint\", \"application_type\"\n",
    "}\n",
    "sec_app_prefix = \"sec_app_\"\n",
    "\n",
    "months_since = {\n",
    "    \"mths_since_last_record\",\"mths_since_last_major_derog\",\"mths_since_last_delinq\",\n",
    "    \"mths_since_recent_revol_delinq\",\"mths_since_recent_bc_dlq\",\"mths_since_recent_inq\",\n",
    "    \"mths_since_rcnt_il\",\"mths_since_recent_bc\",\n",
    "}\n",
    "\n",
    "util_counts = {\n",
    "    \"il_util\",\"all_util\",\"bc_util\",\"percent_bc_gt_75\",\"bc_open_to_buy\",\n",
    "    \"open_acc_6m\",\"open_il_12m\",\"open_il_24m\",\"open_rv_12m\",\"open_rv_24m\",\n",
    "    \"inq_fi\",\"inq_last_12m\",\"total_bal_il\",\"total_cu_tl\",\"max_bal_bc\"\n",
    "}\n",
    "\n",
    "def prune_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = df.columns\n",
    "\n",
    "    # Drop post-origination and joint/sec_app due to not useful information\n",
    "    to_drop = set(cols) & (post_orig | joint_like | {c for c in cols if c.startswith(sec_app_prefix)})\n",
    "    df = df.drop(columns=list(to_drop), errors=\"ignore\").copy()\n",
    "\n",
    "    # Impute + flag for months-since (999) and util/count (0)\n",
    "    # for c in (months_since & set(df.columns)):\n",
    "    for c in (months_since):\n",
    "        # df[c+\"_isna\"] = df[c].isna().astype(\"int8\")\n",
    "        df[c] = df[c].fillna(999)\n",
    "\n",
    "    # for c in (util_counts & set(df.columns)):\n",
    "    for c in (util_counts):\n",
    "        # df[c+\"_isna\"] = df[c].isna().astype(\"int8\")\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example:\n",
    "train_clean = prune_and_impute(train_pre_eda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage (exclude target if you want):\n",
    "profile_train = quick_profile(train_clean, exclude=[\"target\"])\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "profile_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2f313",
   "metadata": {},
   "source": [
    "Is not necesary to segment by user with buró information nor by application_type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b239c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # emp_length: map to years (float). Handles '10+ years', '< 1 year', 'n/a', NaN\n",
    "    if \"emp_length\" in out:\n",
    "        m = {\n",
    "            \"10+ years\": 10.0, \"9 years\": 9.0, \"8 years\": 8.0, \"7 years\": 7.0,\n",
    "            \"6 years\": 6.0, \"5 years\": 5.0, \"4 years\": 4.0, \"3 years\": 3.0,\n",
    "            \"2 years\": 2.0, \"1 year\": 1.0, \"< 1 year\": 0.5, \"n/a\": np.nan\n",
    "        }\n",
    "        out[\"emp_length_yrs\"] = out[\"emp_length\"].map(m).astype(\"float32\")\n",
    "\n",
    "    # revol_util: strip '%' and cast to float\n",
    "    if \"revol_util\" in out:\n",
    "        out[\"revol_util_pct\"] = (\n",
    "            out[\"revol_util\"].astype(str).str.replace(\"%\",\"\", regex=False)\n",
    "            .replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "            .astype(float)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "train_eda = normalize_types(train_pre_eda)\n",
    "valid_eda = normalize_types(valid_pre_eda)\n",
    "test_eda  = normalize_types(test_pre_eda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cada5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 2A) Zero-impute + flag (examples; add more if needed)\n",
    "    zero_impute = [c for c in [\"open_act_il\", \"il_util\", \"all_util\", \"bc_util\", \"bc_open_to_buy\",\n",
    "                               \"inq_fi\", \"inq_last_12m\", \"total_bal_il\", \"total_cu_tl\", \"max_bal_bc\"]\n",
    "                   if c in out]\n",
    "    for c in zero_impute:\n",
    "        out[c + \"_isna\"] = out[c].isna().astype(\"int8\")\n",
    "        out[c] = out[c].fillna(0)\n",
    "\n",
    "    # 2B) “months since …” sentinel = 999 → add flag\n",
    "    months_since = [c for c in out.columns if c.startswith(\"mths_since_\")]\n",
    "    for c in months_since:\n",
    "        out[c + \"_is999\"] = (out[c] == 999).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "train_eda = add_missing_flags(train_eda)\n",
    "valid_eda = add_missing_flags(valid_eda)\n",
    "test_eda  = add_missing_flags(test_eda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(df, cols, p_low=0.01, p_high=0.99):\n",
    "    out = df.copy()\n",
    "    q = out[cols].quantile([p_low, p_high])\n",
    "    lo, hi = q.iloc[0], q.iloc[1]\n",
    "    for c in cols:\n",
    "        if c in out:\n",
    "            out[c] = out[c].clip(lo[c], hi[c])\n",
    "    return out\n",
    "\n",
    "heavy = [c for c in [\"annual_inc\",\"revol_bal\",\"avg_cur_bal\",\"tot_cur_bal\",\n",
    "                     \"tot_hi_cred_lim\",\"total_rev_hi_lim\",\"total_bal_ex_mort\",\n",
    "                     \"loan_amnt\",\"max_bal_bc\"] if c in train_eda]\n",
    "train_eda = winsorize(train_eda, heavy)\n",
    "valid_eda = winsorize(valid_eda, heavy)\n",
    "test_eda  = winsorize(test_eda,  heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map values: 'non_default' → 0, 'default' → 1\n",
    "train_eda[\"target\"] = train_eda[\"target\"].map({\"non_default\": 0, \"default\": 1}).astype(\"int8\")\n",
    "valid_eda[\"target\"] = valid_eda[\"target\"].map({\"non_default\": 0, \"default\": 1}).astype(\"int8\")\n",
    "test_eda[\"target\"] = test_eda[\"target\"].map({\"non_default\": 0, \"default\": 1}).astype(\"int8\")\n",
    "\n",
    "# Confirm result\n",
    "print(train_eda[\"target\"].value_counts(dropna=False))\n",
    "print(train_eda[\"target\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def psi_num(x_tr, x_va, bins=10):\n",
    "    tr = pd.Series(x_tr).dropna(); va = pd.Series(x_va).dropna()\n",
    "    if tr.empty or va.empty: return 0.0\n",
    "    cuts = tr.quantile(np.linspace(0,1,bins+1)).unique()\n",
    "    if len(cuts) < 3: return 0.0\n",
    "    tr_b = pd.cut(tr, cuts, include_lowest=True)\n",
    "    va_b = pd.cut(va, cuts, include_lowest=True)\n",
    "    p = tr_b.value_counts(normalize=True).sort_index().replace(0,1e-6)\n",
    "    q = va_b.value_counts(normalize=True).reindex(p.index, fill_value=1e-6)\n",
    "    return float(((p - q) * np.log(p / q)).sum())\n",
    "\n",
    "def psi_cat(x_tr, x_va, top_k=20):\n",
    "    tr = pd.Series(x_tr).astype(str); va = pd.Series(x_va).astype(str)\n",
    "    top = tr.value_counts().head(top_k).index\n",
    "    tr = tr.where(tr.isin(top), \"_OTHER_\")\n",
    "    va = va.where(va.isin(top), \"_OTHER_\")\n",
    "    idx = tr.value_counts(normalize=True).index.union(va.value_counts(normalize=True).index)\n",
    "    p = tr.value_counts(normalize=True).reindex(idx, fill_value=0).replace(0,1e-6)\n",
    "    q = va.value_counts(normalize=True).reindex(idx, fill_value=0).replace(0,1e-6)\n",
    "    return float(((p - q) * np.log(p / q)).sum())\n",
    "\n",
    "num_cols = [c for c in train_eda.columns if c not in (\"target\",) and pd.api.types.is_numeric_dtype(train_eda[c])]\n",
    "cat_cols = [c for c in train_eda.columns if c not in (\"target\",) and not pd.api.types.is_numeric_dtype(train_eda[c])]\n",
    "\n",
    "psi_rows = []\n",
    "for c in num_cols:\n",
    "    psi_rows.append({\"feature\": c, \"psi\": psi_num(train_eda[c], valid_eda[c])})\n",
    "for c in cat_cols:\n",
    "    psi_rows.append({\"feature\": c, \"psi\": psi_cat(train_eda[c], valid_eda[c])})\n",
    "\n",
    "psi_df = pd.DataFrame(psi_rows).sort_values(\"psi\", ascending=False)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "psi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_numeric(x, y, bins=10):\n",
    "    df = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    if df.empty: return 0.0\n",
    "    try:\n",
    "        df[\"bin\"] = pd.qcut(df[\"x\"], q=bins, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    g = df.groupby(\"bin\")[\"y\"].agg([\"count\",\"sum\"])\n",
    "    good, bad = g[\"count\"]-g[\"sum\"], g[\"sum\"]\n",
    "    pg, pb = (good/good.sum()).replace(0,1e-6), (bad/bad.sum()).replace(0,1e-6)\n",
    "    woe = np.log(pg/pb)\n",
    "    return float(((pg - pb) * woe).sum())\n",
    "\n",
    "def iv_categorical(x, y, top_k=50):\n",
    "    s = pd.Series(x).astype(str)\n",
    "    top = s.value_counts().head(top_k).index\n",
    "    s = s.where(s.isin(top), \"_OTHER_\")\n",
    "    g = pd.DataFrame({\"x\": s, \"y\": y}).groupby(\"x\")[\"y\"].agg([\"count\",\"sum\"])\n",
    "    good, bad = g[\"count\"]-g[\"sum\"], g[\"sum\"]\n",
    "    pg, pb = (good/good.sum()).replace(0,1e-6), (bad/bad.sum()).replace(0,1e-6)\n",
    "    woe = np.log(pg/pb)\n",
    "    return float(((pg - pb) * woe).sum())\n",
    "\n",
    "iv_rows = []\n",
    "for c in num_cols: iv_rows.append({\"feature\": c, \"iv\": iv_numeric(train_eda[c], train_eda[\"target\"])})\n",
    "for c in cat_cols: iv_rows.append({\"feature\": c, \"iv\": iv_categorical(train_eda[c], train_eda[\"target\"])})\n",
    "\n",
    "iv_df = pd.DataFrame(iv_rows).sort_values(\"iv\", ascending=False)\n",
    "iv_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46ccb2",
   "metadata": {},
   "source": [
    "## Correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c5677",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Pick numeric columns\n",
    "num_cols = [c for c in train_eda.columns\n",
    "            if c != \"target\" and pd.api.types.is_numeric_dtype(train_eda[c])]\n",
    "\n",
    "# 2) Spearman absolute correlations\n",
    "corr = train_eda[num_cols].corr(method=\"spearman\").abs()\n",
    "\n",
    "# ----- Option A: show full matrix -----\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(corr.values, aspect=\"auto\", vmin=0, vmax=1)\n",
    "plt.colorbar(label=\"|Spearman ρ|\")\n",
    "plt.xticks(ticks=np.arange(len(num_cols)), labels=num_cols, rotation=90, fontsize=6)\n",
    "plt.yticks(ticks=np.arange(len(num_cols)), labels=num_cols, fontsize=6)\n",
    "plt.title(\"Correlation Heat Map (|Spearman|) — train\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Option B (optional): top-N by max off-diagonal correlation (for readability) -----\n",
    "N = 30  # change to taste\n",
    "off_diag = corr.copy()\n",
    "np.fill_diagonal(off_diag.values, 0)\n",
    "top_feats = off_diag.max(axis=1).sort_values(ascending=False).head(N).index.tolist()\n",
    "corr_top = corr.loc[top_feats, top_feats]\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(corr_top.values, aspect=\"auto\", vmin=0, vmax=1)\n",
    "plt.colorbar(label=\"|Spearman ρ|\")\n",
    "plt.xticks(ticks=np.arange(len(top_feats)), labels=top_feats, rotation=90, fontsize=7)\n",
    "plt.yticks(ticks=np.arange(len(top_feats)), labels=top_feats, fontsize=7)\n",
    "plt.title(f\"Correlation Heat Map (Top {N} by max |ρ|)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d500a9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) thresholds\n",
    "IV_MIN = 0.02\n",
    "PSI_MAX = 0.25\n",
    "\n",
    "# 2) base keep set by IV & PSI\n",
    "iv_keep = set(iv_df.loc[iv_df.iv >= IV_MIN, \"feature\"])\n",
    "psi_keep = set(psi_df.loc[psi_df.psi <= PSI_MAX, \"feature\"])\n",
    "base_keep = (iv_keep & psi_keep)\n",
    "\n",
    "# 3) add engineered replacements you plan to use\n",
    "base_keep |= {\"fico_mid\", \"sec_app_fico_mid\", \"revol_util_pct\", \"emp_length_yrs\"}\n",
    "\n",
    "# 4) drop explicit unstable ones (from PSI list)\n",
    "explicit_drop = {\n",
    "    \"max_bal_bc\",\"all_util\",\"total_bal_il\",\"il_util\",\"open_act_il\",\"mths_since_last_record\",\"sec_app_earliest_cr_line\"\n",
    "}\n",
    "base_keep -= explicit_drop\n",
    "\n",
    "# 5) optional: choose representative from highly correlated groups using IV\n",
    "num_candidates = [c for c in base_keep if c in train_eda.columns and pd.api.types.is_numeric_dtype(train_eda[c])]\n",
    "corr = train_eda[num_candidates].corr(method=\"spearman\").abs()\n",
    "\n",
    "# greedy prune: sort by IV desc, keep a column, drop any others with |rho|>=0.85 to it\n",
    "iv_rank = iv_df.set_index(\"feature\").iv.to_dict()\n",
    "selected, dropped = [], set()\n",
    "for f in sorted(num_candidates, key=lambda x: -iv_rank.get(x, 0.05)):\n",
    "    if f in dropped: \n",
    "        continue\n",
    "    selected.append(f)\n",
    "    highly_corr = corr.index[(corr[f] >= 0.85) & (corr.index != f)].tolist()\n",
    "    dropped.update(highly_corr)\n",
    "\n",
    "shortlist = sorted((set(selected) | (base_keep - set(num_candidates))) - dropped)\n",
    "print(\"Shortlist (n={}):\".format(len(shortlist)))\n",
    "print(shortlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train_eda\n",
    "final_shortlist = [c for c in [\n",
    " 'acc_open_past_24mths','annual_inc','annual_inc_joint','avg_cur_bal','bc_open_to_buy',\n",
    " 'dti','dti_joint','emp_length_yrs','fico_mid','home_ownership','inq_last_6mths',\n",
    " 'loan_amnt','mo_sin_old_rev_tl_op','mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl','mort_acc',\n",
    " 'mths_since_rcnt_il','mths_since_recent_bc','mths_since_recent_inq','num_actv_rev_tl',\n",
    " 'num_tl_op_past_12m','open_acc_6m','open_il_24m','open_rv_12m','open_rv_24m',\n",
    " 'percent_bc_gt_75','revol_util_pct','sec_app_fico_mid','sec_app_inq_last_6mths',\n",
    " 'sec_app_mort_acc','sec_app_revol_util','total_bc_limit','verification_status'\n",
    "] if c in train_f.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b52de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_eda.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_f = valid_eda\n",
    "def seg_table(train, valid, col, target='target', top_k=10):\n",
    "    top = train[col].value_counts().head(top_k).index\n",
    "    t = (train.assign(_grp=train[col].where(train[col].isin(top), '_OTHER_'))\n",
    "               .groupby('_grp')[target].agg(['mean','count']))\n",
    "    v = (valid.assign(_grp=valid[col].where(valid[col].isin(top), '_OTHER_'))\n",
    "               .groupby('_grp')[target].agg(['mean','count']))\n",
    "    out = (t.rename(columns={'mean':'train_rate','count':'train_n'})\n",
    "             .join(v.rename(columns={'mean':'valid_rate','count':'valid_n'}), how='outer').fillna(0))\n",
    "    return out.sort_values('train_n', ascending=False)\n",
    "\n",
    "for c in ['purpose','home_ownership','verification_status','addr_state']:\n",
    "    if c in train_f:\n",
    "        display(seg_table(train_f, valid_f, c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_ok = psi_df[psi_df.feature.isin(final_shortlist)].sort_values('psi', ascending=False)\n",
    "psi_ok.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8639fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_f.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "train_f[final_shortlist].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Create output folders\n",
    "out = Path(\"data/eda\"); out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as Parquet with fastparquet (preserves dtypes nicely)\n",
    "train_eda[final_shortlist].to_parquet(out / \"train.parquet\", engine=\"fastparquet\", index=False)\n",
    "valid_eda[final_shortlist].to_parquet(out / \"valid.parquet\", engine=\"fastparquet\", index=False)\n",
    "test_eda[final_shortlist].to_parquet( out / \"test.parquet\",  engine=\"fastparquet\", index=False)\n",
    "\n",
    "print(\"Saved to:\", list(out.glob(\"*.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= SETUP =======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline          # <-- mover arriba para usar Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve\n",
    "\n",
    "import xgboost as xgb                          # <--- módulo, NO lo sobrescribas\n",
    "\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "# Split X/y\n",
    "X_tr = train_eda[final_shortlist].copy()\n",
    "y_tr = train_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "X_va = valid_eda[final_shortlist].copy()\n",
    "y_va = valid_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "X_ot = test_eda[final_shortlist].copy()\n",
    "y_ot = test_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "# Identificar categóricas y numéricas\n",
    "cat_candidates = {\"home_ownership\", \"verification_status\"}\n",
    "cat_feats = [c for c in final_shortlist if c in cat_candidates]\n",
    "num_feats = [c for c in final_shortlist if c not in cat_candidates]\n",
    "\n",
    "# ======= PREPROCESSING =======\n",
    "num_transformer = SimpleImputer(strategy=\"median\")\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_feats),\n",
    "        (\"cat\", cat_transformer, cat_feats),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "prep_pipe = Pipeline([(\"prep\", preprocess)])\n",
    "prep_pipe.fit(X_tr)\n",
    "\n",
    "Xtr = prep_pipe.transform(X_tr)\n",
    "Xva = prep_pipe.transform(X_va)\n",
    "Xot = prep_pipe.transform(X_ot)\n",
    "\n",
    "def get_feature_names(prep_pipe):\n",
    "    ct = prep_pipe.named_steps[\"prep\"]\n",
    "    out_names = []\n",
    "    # num\n",
    "    out_names += list(ct.transformers_[0][2])  # num_feats\n",
    "    # cat\n",
    "    ohe = ct.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "    cat_ohe_names = list(ohe.get_feature_names_out(cat_feats))\n",
    "    out_names += cat_ohe_names\n",
    "    return out_names\n",
    "\n",
    "feature_names_out = get_feature_names(prep_pipe)\n",
    "n_num = len(num_feats)\n",
    "n_cat_out = len(feature_names_out) - n_num\n",
    "\n",
    "# ======= MONOTONICIDAD =======\n",
    "mono_map = {\n",
    "    \"dti\": +1, \"inq_last_6mths\": +1, \"percent_bc_gt_75\": +1,\n",
    "    \"revol_util_pct\": +1, \"loan_amnt\": +1, \"num_tl_op_past_12m\": +1,\n",
    "    \"open_rv_12m\": +1, \"open_rv_24m\": +1,\n",
    "    \"fico_mid\": -1, \"annual_inc\": -1, \"bc_open_to_buy\": -1,\n",
    "    \"avg_cur_bal\": -1, \"total_bc_limit\": -1,\n",
    "    \"mths_since_recent_inq\": -1, \"mths_since_rcnt_il\": -1, \"mths_since_recent_bc\": -1,\n",
    "}\n",
    "mono_vec_num = [mono_map.get(f, 0) for f in num_feats]\n",
    "mono_vec = mono_vec_num + [0] * n_cat_out\n",
    "monotone_constraints = \"(\" + \",\".join(str(int(v)) for v in mono_vec) + \")\"\n",
    "\n",
    "# ======= IMBALANCE =======\n",
    "pos = (y_tr == 1).sum()\n",
    "neg = (y_tr == 0).sum()\n",
    "scale_pos_weight = (neg / max(pos, 1))\n",
    "\n",
    "# ======= XGBOOST (API nativa) + EARLY STOPPING =======\n",
    "xgb_params_native = {\n",
    "    \"eta\": 0.03,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 20,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"alpha\": 5.0,          # reg_alpha\n",
    "    \"lambda\": 10.0,        # reg_lambda\n",
    "    \"tree_method\": \"hist\", # o \"gpu_hist\"\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"eval_metric\": [\"auc\", \"aucpr\", \"logloss\"],\n",
    "    \"monotone_constraints\": monotone_constraints,\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(Xtr, label=y_tr, feature_names=feature_names_out)\n",
    "dvalid = xgb.DMatrix(Xva, label=y_va, feature_names=feature_names_out)\n",
    "doot   = xgb.DMatrix(Xot, label=y_ot, feature_names=feature_names_out)\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "bst = xgb.train(\n",
    "    params=xgb_params_native,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=5000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ======= CALIBRACIÓN ISOTÓNICA (sobre VALID) =======\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "p_tr_raw = bst.predict(dtrain, iteration_range=(0, bst.best_iteration + 1))\n",
    "p_va_raw = bst.predict(dvalid, iteration_range=(0, bst.best_iteration + 1))\n",
    "p_ot_raw = bst.predict(doot,   iteration_range=(0, bst.best_iteration + 1))\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_va_raw, y_va)\n",
    "\n",
    "p_tr = iso.predict(p_tr_raw)\n",
    "p_va = iso.predict(p_va_raw)\n",
    "p_ot = iso.predict(p_ot_raw)\n",
    "\n",
    "# ======= MÉTRICAS =======\n",
    "def ks_score(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return np.max(np.abs(tpr - fpr))\n",
    "\n",
    "def lift_at_k(y_true, y_score, k=0.1):\n",
    "    n = len(y_true)\n",
    "    k_n = max(1, int(np.floor(k * n)))\n",
    "    order = np.argsort(-y_score)\n",
    "    top = y_true[order][:k_n]\n",
    "    base_rate = y_true.mean()\n",
    "    return (top.mean() / base_rate) if base_rate > 0 else np.nan\n",
    "\n",
    "def psi(expected, actual, bins=10, eps=1e-6):\n",
    "    cuts = np.quantile(expected, np.linspace(0, 1, bins + 1))\n",
    "    cuts[0], cuts[-1] = -np.inf, np.inf\n",
    "    e = np.histogram(expected, bins=cuts)[0] / len(expected)\n",
    "    a = np.histogram(actual, bins=cuts)[0] / len(actual)\n",
    "    e = np.clip(e, eps, None); a = np.clip(a, eps, None)\n",
    "    return np.sum((a - e) * np.log(a / e))\n",
    "\n",
    "def all_metrics(y, p, name=\"set\", k_list=(0.05, 0.1, 0.2)):\n",
    "    out = {}\n",
    "    out[f\"AUC_{name}\"] = roc_auc_score(y, p)\n",
    "    out[f\"PR_AUC_{name}\"] = average_precision_score(y, p)\n",
    "    out[f\"Brier_{name}\"] = brier_score_loss(y, p)\n",
    "    out[f\"KS_{name}\"] = ks_score(y, p)\n",
    "    for k in k_list:\n",
    "        out[f\"Lift@{int(k*100)}%_{name}\"] = lift_at_k(y, p, k)\n",
    "    return out\n",
    "\n",
    "m_tr = all_metrics(y_tr, p_tr, name=\"TR\")\n",
    "m_va = all_metrics(y_va, p_va, name=\"VA\")\n",
    "m_ot = all_metrics(y_ot, p_ot, name=\"OOT\")\n",
    "\n",
    "print(pd.DataFrame([m_tr | m_va | m_ot]).T.sort_index())\n",
    "print(f\"PSI(score) VAL→OOT: {psi(p_va, p_ot):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92567361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Logistic Regression (L2) con calibración =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, roc_curve\n",
    ")\n",
    "\n",
    "# ---------- Datos ----------\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "X_tr_lr = train_eda[final_shortlist].copy()\n",
    "y_tr_lr = train_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "X_va_lr = valid_eda[final_shortlist].copy()\n",
    "y_va_lr = valid_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "X_ot_lr = test_eda[final_shortlist].copy()\n",
    "y_ot_lr = test_eda[TARGET_COL].astype(int).values\n",
    "\n",
    "# ---------- Features num/cat ----------\n",
    "cat_candidates = {\"home_ownership\", \"verification_status\"}\n",
    "cat_feats_lr = [c for c in final_shortlist if c in cat_candidates]\n",
    "num_feats_lr = [c for c in final_shortlist if c not in cat_candidates]\n",
    "\n",
    "# ---------- Preprocesamiento ----------\n",
    "num_transformer_lr = SimpleImputer(strategy=\"median\")\n",
    "cat_transformer_lr = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocess_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer_lr, num_feats_lr),\n",
    "        (\"cat\", cat_transformer_lr, cat_feats_lr),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "prep_pipe_lr = Pipeline([(\"prep\", preprocess_lr)])\n",
    "prep_pipe_lr.fit(X_tr_lr)\n",
    "\n",
    "Xtr_lr = prep_pipe_lr.transform(X_tr_lr)\n",
    "Xva_lr = prep_pipe_lr.transform(X_va_lr)\n",
    "Xot_lr = prep_pipe_lr.transform(X_ot_lr)\n",
    "\n",
    "# ---------- Entrenamiento LR ----------\n",
    "# class_weight=\"balanced\" para manejar desbalanceo\n",
    "lr = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=5000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=None\n",
    ")\n",
    "lr.fit(Xtr_lr, y_tr_lr)\n",
    "\n",
    "# ---------- Probabilidades crudas ----------\n",
    "p_tr_raw_lr = lr.predict_proba(Xtr_lr)[:, 1]\n",
    "p_va_raw_lr = lr.predict_proba(Xva_lr)[:, 1]\n",
    "p_ot_raw_lr = lr.predict_proba(Xot_lr)[:, 1]\n",
    "\n",
    "# ---------- Calibración Isotónica (en VALID) ----------\n",
    "iso_lr = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso_lr.fit(p_va_raw_lr, y_va_lr)\n",
    "\n",
    "p_tr_lr = iso_lr.predict(p_tr_raw_lr)\n",
    "p_va_lr = iso_lr.predict(p_va_raw_lr)\n",
    "p_ot_lr = iso_lr.predict(p_ot_raw_lr)\n",
    "\n",
    "# ---------- Métricas (mismas que XGB) ----------\n",
    "def ks_score(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return np.max(np.abs(tpr - fpr))\n",
    "\n",
    "def lift_at_k(y_true, y_score, k=0.1):\n",
    "    n = len(y_true)\n",
    "    k_n = max(1, int(np.floor(k * n)))\n",
    "    order = np.argsort(-y_score)\n",
    "    top = y_true[order][:k_n]\n",
    "    base_rate = y_true.mean()\n",
    "    return (top.mean() / base_rate) if base_rate > 0 else np.nan\n",
    "\n",
    "def psi(expected, actual, bins=10, eps=1e-6):\n",
    "    cuts = np.quantile(expected, np.linspace(0, 1, bins + 1))\n",
    "    cuts[0], cuts[-1] = -np.inf, np.inf\n",
    "    e = np.histogram(expected, bins=cuts)[0] / len(expected)\n",
    "    a = np.histogram(actual, bins=cuts)[0] / len(actual)\n",
    "    e = np.clip(e, eps, None); a = np.clip(a, eps, None)\n",
    "    return np.sum((a - e) * np.log(a / e))\n",
    "\n",
    "def all_metrics(y, p, name=\"set\", k_list=(0.05, 0.1, 0.2)):\n",
    "    out = {}\n",
    "    out[f\"AUC_{name}\"] = roc_auc_score(y, p)\n",
    "    out[f\"PR_AUC_{name}\"] = average_precision_score(y, p)\n",
    "    out[f\"Brier_{name}\"] = brier_score_loss(y, p)\n",
    "    out[f\"KS_{name}\"] = ks_score(y, p)\n",
    "    for k in k_list:\n",
    "        out[f\"Lift@{int(k*100)}%_{name}\"] = lift_at_k(y, p, k)\n",
    "    return out\n",
    "\n",
    "m_tr_lr = all_metrics(y_tr_lr, p_tr_lr, name=\"TR\")\n",
    "m_va_lr = all_metrics(y_va_lr, p_va_lr, name=\"VA\")\n",
    "m_ot_lr = all_metrics(y_ot_lr, p_ot_lr, name=\"OOT\")\n",
    "\n",
    "print(\"==== LOGISTIC REGRESSION (calibrada) ====\")\n",
    "print(pd.DataFrame([m_tr_lr | m_va_lr | m_ot_lr]).T.sort_index())\n",
    "\n",
    "# ---------- PSI del score (VAL → OOT) ----------\n",
    "psi_val_oot_lr = psi(p_va_lr, p_ot_lr)\n",
    "print(f\"\\nPSI(score) VAL→OOT (LR): {psi_val_oot_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1195e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SHAP para XGBoost (bst) =====\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Usa un subconjunto para velocidad si tu valid es grande\n",
    "N_SAMPLE = min(8000, Xva.shape[0])\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(Xva.shape[0], N_SAMPLE, replace=False)\n",
    "Xva_shap = Xva[idx]\n",
    "yva_shap = y_va[idx]\n",
    "\n",
    "# Intenta nueva API de SHAP; si no, usa la antigua\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(bst)                   # booster nativo\n",
    "    shap_out = explainer(Xva_shap)                       # -> Explanation\n",
    "    shap_values = shap_out.values                        # (n, p)\n",
    "    base_value = shap_out.base_values.mean()\n",
    "except Exception:\n",
    "    explainer = shap.TreeExplainer(bst)\n",
    "    shap_values = explainer.shap_values(Xva_shap)        # (n, p)\n",
    "    base_value = explainer.expected_value\n",
    "\n",
    "# ===== 1) Beeswarm (todas las columnas transformadas) =====\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, Xva_shap, feature_names=feature_names_out, show=False)\n",
    "plt.title(\"SHAP beeswarm — Valid sample\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 2) Bar summary (importancia global media |SHAP|) =====\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, Xva_shap, feature_names=feature_names_out, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP global importance (mean |SHAP|) — Valid sample\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 3) Dependence plots para top-N =====\n",
    "# top por mean(|shap|):\n",
    "mean_abs = np.mean(np.abs(shap_values), axis=0)\n",
    "order = np.argsort(-mean_abs)\n",
    "TOP_N = 12  # ajusta a gusto\n",
    "top_idx = order[:TOP_N]\n",
    "for j in top_idx:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.dependence_plot(\n",
    "        j, shap_values, Xva_shap, feature_names=feature_names_out,\n",
    "        interaction_index=\"auto\", show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP dependence — {feature_names_out[j]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===== 4) (Opcional) Agregar dummies OHE por variable original =====\n",
    "# Las columnas OHE vienen como: \"<feature>_<categoria>\".\n",
    "# Agrupamos todas las que empiezan por \"<feature>_\" bajo el nombre original.\n",
    "cat_feats_set = set(cat_feats)  # p.ej. {'home_ownership','verification_status'}\n",
    "\n",
    "def original_name(colname: str) -> str:\n",
    "    # Si es OHE, empieza por alguno de los cat_feats + '_'\n",
    "    for feat in cat_feats_set:\n",
    "        prefix = f\"{feat}_\"\n",
    "        if colname.startswith(prefix):\n",
    "            return feat\n",
    "    return colname  # numéricas (o cat sin OHE)\n",
    "\n",
    "orig_names = np.array([original_name(c) for c in feature_names_out])\n",
    "\n",
    "# mean(|SHAP|) por columna → agregamos por nombre original\n",
    "df_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"orig\": orig_names,\n",
    "        \"col\": feature_names_out,\n",
    "        \"mean_abs_shap\": mean_abs\n",
    "    })\n",
    "    .groupby(\"orig\", as_index=False)[\"mean_abs_shap\"].sum()\n",
    "    .sort_values(\"mean_abs_shap\", ascending=False)\n",
    ")\n",
    "\n",
    "# Barplot agregado por variable original\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_imp[\"orig\"].iloc[:20][::-1], df_imp[\"mean_abs_shap\"].iloc[:20][::-1])\n",
    "plt.xlabel(\"Sum mean |SHAP| (aggregated OHE)\")\n",
    "plt.title(\"Global importance by original feature (top 20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee82536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c58a7220",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Config ---\n",
    "TARGET = \"target\"\n",
    "RS = 42\n",
    "FOLDS_MAX = 5\n",
    "\n",
    "# --- Target: explicit mapping (default -> 1, others -> 0) ---\n",
    "y_raw = train_pre_eda[TARGET].astype(str).str.lower().str.replace(r\"\\s+\", \"\", regex=True)\n",
    "y = y_raw.eq(\"default\").astype(int).values\n",
    "\n",
    "# basic guard\n",
    "n_pos, n_neg = int((y == 1).sum()), int((y == 0).sum())\n",
    "if n_pos == 0 or n_neg == 0:\n",
    "    raise ValueError(f\"Target has a single class after mapping. pos={n_pos}, neg={n_neg}\")\n",
    "\n",
    "# --- Features: one-hot for robustness ---\n",
    "X = train_pre_eda.drop(columns=[TARGET]).copy()\n",
    "X_test = test_pre_eda.copy()\n",
    "\n",
    "both = pd.concat([X, X_test], axis=0, ignore_index=True)\n",
    "cat_cols = both.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "both = pd.get_dummies(both, columns=cat_cols, dummy_na=True)\n",
    "\n",
    "X = both.iloc[:len(X)].reset_index(drop=True)\n",
    "X_test = both.iloc[len(X):].reset_index(drop=True)\n",
    "\n",
    "# make sure no infs\n",
    "for df in (X, X_test):\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# --- Class weight ---\n",
    "spw = float(max((n_neg / n_pos), 1.0))\n",
    "\n",
    "# --- Model params (simple) ---\n",
    "params = dict(\n",
    "    tree_method=\"hist\",\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.08,\n",
    "    max_depth=6,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2.0,\n",
    "    reg_lambda=1.5,\n",
    "    random_state=RS,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=spw,\n",
    ")\n",
    "\n",
    "# --- Folds (cap by minority count) ---\n",
    "min_count = min(n_pos, n_neg)\n",
    "FOLDS = max(2, min(FOLDS_MAX, min_count))\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RS)\n",
    "\n",
    "oof = np.zeros(len(X), dtype=float)\n",
    "test_pred = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "# Use NumPy arrays to avoid feature name issues\n",
    "X_np = X.to_numpy(dtype=float, copy=False)\n",
    "X_test_np = X_test.to_numpy(dtype=float, copy=False)\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X_np, y), 1):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_np[tr], y[tr])\n",
    "    proba = model.predict_proba(X_np[va])[:, 1]\n",
    "    oof[va] = proba\n",
    "    test_pred += model.predict_proba(X_test_np)[:, 1] / FOLDS\n",
    "    print(f\"[Fold {fold}] AUC={roc_auc_score(y[va], proba):.4f}\")\n",
    "\n",
    "print(f\"\\n[OOF] AUC={roc_auc_score(y, oof):.4f}\")\n",
    "\n",
    "# --- Submission ---\n",
    "ids = test_pre_eda[\"id\"].values if \"id\" in test_pre_eda.columns else np.arange(len(test_pre_eda))\n",
    "submission = pd.DataFrame({\"id\": ids, \"score\": test_pred, \"pred\": (test_pred >= 0.5).astype(int)})\n",
    "print(submission.head())\n",
    "# submission.to_csv(\"submission_xgb.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoring-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
